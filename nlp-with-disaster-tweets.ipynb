{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# NLP with Disaster Tweets\n\nThis notebook contains code for training BERT using Pytorch and Hugging Face on the dataset used in the Kaggle Competition Natural Language Processing with Disaster Tweets (https://www.kaggle.com/competitions/nlp-getting-started/)\n\n# Dataset\n\n## Files\n\ntrain.csv - the training set\n\ntest.csv - the test set\n\nsample_submission.csv - a sample submission file to submit the predictions on Kaggle\n\n## Columns\n\nid - id for each tweet\n\ntext - the text of the tweet\n\nlocation - the location the tweet was sent from\n\nkeyword - a particular keyword from the tweet\n\ntarget - in train.csv only, this denotes whether a tweet is about a real disaster (1) or not (0)\n\n### The goal is to predict if the tweet is about a disaster or not.","metadata":{}},{"cell_type":"code","source":"# Import Libraries\nimport os\nimport gc\nimport sys\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\nimport transformers\nfrom torch.utils.data import DataLoader, Dataset","metadata":{"execution":{"iopub.status.busy":"2022-04-02T23:00:10.073315Z","iopub.execute_input":"2022-04-02T23:00:10.073667Z","iopub.status.idle":"2022-04-02T23:00:12.483823Z","shell.execute_reply.started":"2022-04-02T23:00:10.073576Z","shell.execute_reply":"2022-04-02T23:00:12.483119Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"#Model hyperparameter configs\nclass CONFIG:\n    MODEL_PATH = './model/'\n    SAVE_EVERT = 10\n    EPOCHS = 10\n    BATCH_SIZE = 16\n    LEARNING_RATE = 1e-5\n    TRAIN_TEST_SPLIT = 0.3","metadata":{"execution":{"iopub.status.busy":"2022-04-02T19:14:05.662358Z","iopub.execute_input":"2022-04-02T19:14:05.662611Z","iopub.status.idle":"2022-04-02T19:14:05.667417Z","shell.execute_reply.started":"2022-04-02T19:14:05.662582Z","shell.execute_reply":"2022-04-02T19:14:05.666170Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# BERT CLass\nclass BERT(nn.Module):\n    def __init__(self, bert_model_name, num_labels, dropout=0.1, freeze_bert=True):\n        super(BERT, self).__init__()\n        # Import BERT from HuggingFace Transformer library\n        self.bert = transformers.BertModel.from_pretrained(bert_model_name)\n        # Freeze weights of Pretrained BERT Model. These weights will not be\n        # updated during training.\n        if freeze_bert:\n            for param in self.bert.parameters():\n                param.requires_grad = False\n        # Add Dropout Layer\n        self.dropout = nn.Dropout(dropout)\n        # Add a Classifier Layer for finetune the model and get the output=number of labels.\n        self.classifier = nn.Linear(768, num_labels)\n        self.num_labels = num_labels\n    def forward(self, input_ids, attention_mask=None, token_type_ids=None):\n        # Forward Function to pass the data.\n        # Pass the data thriugh BERT\n        _, pooled_output = self.bert(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, return_dict=False)\n        # Add Dropout Layer to the BERT Output\n        pooled_output = self.dropout(pooled_output)\n        # Get Classifier Output Logits\n        logits = self.classifier(pooled_output)\n        return logits","metadata":{"execution":{"iopub.status.busy":"2022-04-02T19:14:43.336093Z","iopub.execute_input":"2022-04-02T19:14:43.336669Z","iopub.status.idle":"2022-04-02T19:14:43.343914Z","shell.execute_reply.started":"2022-04-02T19:14:43.336628Z","shell.execute_reply":"2022-04-02T19:14:43.343229Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"class DisasterTweetsDataset(Dataset):\n    def __init__(self, df, tokenizer, max_len=512, train=True):\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n        self.df = df\n        self.train = train\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        # Get Text Data\n        text = str(self.df.iloc[idx][\"text\"])\n        if self.train:\n            # Get labels\n            targets = torch.tensor(self.df.iloc[idx][\"target\"], dtype=torch.long)\n        \n        # Use the BERT Tokenizer on the Inputs.\n        # Add special tokens for start and end of text, pad the input texts to get them in equal length,\n        # Get attention mask on the padded inputs, get token_type_ids and return the outputs in the form \n        # of Pytorch Tensor.\n        inputs = self.tokenizer.encode_plus(\n            text,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            pad_to_max_length=True,\n            return_attention_mask=True,\n            return_token_type_ids=True,\n            return_tensors=\"pt\",\n        )\n        input_ids, attention_mask, token_type_ids = inputs[\"input_ids\"], inputs[\"attention_mask\"], inputs[\"token_type_ids\"]\n        # In case of training, there is an additional targets output.\n        if self.train:\n            return {\n                \"input_ids\": input_ids,\n                \"attention_mask\": attention_mask,\n                \"token_type_ids\": token_type_ids,\n                \"targets\": targets,\n                }\n        else:\n            return {\n                \"input_ids\": input_ids,\n                \"attention_mask\": attention_mask,\n                \"token_type_ids\": token_type_ids,\n                }\n","metadata":{"execution":{"iopub.status.busy":"2022-04-02T19:14:43.901979Z","iopub.execute_input":"2022-04-02T19:14:43.902699Z","iopub.status.idle":"2022-04-02T19:14:43.913451Z","shell.execute_reply.started":"2022-04-02T19:14:43.902660Z","shell.execute_reply":"2022-04-02T19:14:43.912762Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# Check if CUDA is available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Device:\", device)\nprint(\"loading data\")\n# Get Train and Validation Data. Get the Train-Val split from the CONFIG.\ndf = pd.read_csv(\"../input/nlp-getting-started/train.csv\")\ntrain_df, valid_df = train_test_split(df, test_size=CONFIG.TRAIN_TEST_SPLIT, random_state=42)\ntest_df = pd.read_csv(\"../input/nlp-getting-started/test.csv\")\n# Get the sample submission to submit the prediction to Kaggle.\nsub = pd.read_csv(\"../input/nlp-getting-started/sample_submission.csv\")","metadata":{"execution":{"iopub.status.busy":"2022-04-02T19:14:45.352146Z","iopub.execute_input":"2022-04-02T19:14:45.352394Z","iopub.status.idle":"2022-04-02T19:14:45.488335Z","shell.execute_reply.started":"2022-04-02T19:14:45.352366Z","shell.execute_reply":"2022-04-02T19:14:45.487617Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# Load the bert model to the device. Number of labels is 1 because there is one output field that can be \n# 0 or 1 i.e. it is a disaster (1) or not (0).\nmodel = BERT(\"../input/huggingface-bert/bert-base-cased\", num_labels=1, dropout=0.1).to(device)\n# Get tokenizer output.\ntokenizer = transformers.BertTokenizer.from_pretrained(\"../input/huggingface-bert/bert-base-cased\")\n# Get train, valid and test datasets\ntrain_dataset = DisasterTweetsDataset(train_df, tokenizer, max_len=512, train=True)\nvalid_dataset = DisasterTweetsDataset(valid_df, tokenizer, max_len=512, train=True)\ntest_dataset = DisasterTweetsDataset(test_df, tokenizer, max_len=512, train=False)\n# Use AdamW Optimizer\noptimizer = transformers.AdamW(model.parameters(), lr=2e-5)\n# Use Learning rate scheduler\nscheduler = transformers.get_linear_schedule_with_warmup(\n    optimizer,\n    num_warmup_steps=0,\n    num_training_steps=len(train_dataset) * 5,\n)\n# Loss Function\ncriterion = nn.BCEWithLogitsLoss()","metadata":{"execution":{"iopub.status.busy":"2022-04-02T19:14:48.004170Z","iopub.execute_input":"2022-04-02T19:14:48.004751Z","iopub.status.idle":"2022-04-02T19:15:01.673569Z","shell.execute_reply.started":"2022-04-02T19:14:48.004707Z","shell.execute_reply":"2022-04-02T19:15:01.672844Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"def train_one_epoch(model, optimizer, scheduler, train_dataloader, device):\n    # Call the model training function.\n    model.train()\n    # Training Loss\n    total_loss = 0\n    bar = tqdm(enumerate(train_dataloader), total=len(train_dataloader))\n    for i, data in bar:\n        # Zero grad for optimizer before starting the training batch.\n        optimizer.zero_grad()\n        # Pass the input_ids, attention_mask, token_type_ids to the model.\n        input_ids = data[\"input_ids\"].to(device).squeeze(1)\n        attention_mask = data[\"attention_mask\"].to(device).squeeze(1)\n        token_type_ids = data[\"token_type_ids\"].to(device).squeeze(1)\n        targets = data[\"targets\"].to(device).unsqueeze(1)\n        out = model(input_ids, attention_mask, token_type_ids)\n        # Get the model loss.\n        loss = criterion(out, targets.float())\n        bar.set_postfix({\n                \"Train Loss\": \"{:.6f}\".format(abs(loss)),}\n        )\n        # Backpropagate the loss.\n        loss.backward()\n        # Update the model parameters.\n        optimizer.step()\n        # Update the learning rate scheduler.\n        scheduler.step()\n        # Add the batch loss to the epoch loss.\n        total_loss += loss.item()\n    # Normalize the loss.\n    return total_loss / len(train_dataloader)","metadata":{"execution":{"iopub.status.busy":"2022-04-02T19:15:12.182425Z","iopub.execute_input":"2022-04-02T19:15:12.182861Z","iopub.status.idle":"2022-04-02T19:15:12.197896Z","shell.execute_reply.started":"2022-04-02T19:15:12.182816Z","shell.execute_reply":"2022-04-02T19:15:12.196387Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"def valid_one_epoch(model, valid_dataloader, device):\n    # Change the model to eval mode.\n    model.eval()\n    # Validation Loss\n    total_loss = 0\n    with torch.no_grad():\n        bar = tqdm(enumerate(valid_dataloader), total=len(valid_dataloader))\n        for i, data in bar:\n            # Pass the input_ids, attention_mask, token_type_ids to the model.\n            input_ids = data[\"input_ids\"].to(device).squeeze(1)\n            attention_mask = data[\"attention_mask\"].to(device).squeeze(1)\n            token_type_ids = data[\"token_type_ids\"].to(device).squeeze(1)\n            targets = data[\"targets\"].to(device).unsqueeze(1)\n            out = model(input_ids, attention_mask, token_type_ids)\n            # Get the model loss.\n            loss = criterion(out, targets.float())\n            bar.set_postfix({\n                \"Valid Loss\": \"{:.6f}\".format(abs(loss)),}\n            )\n            # Add the batch loss to the epoch loss.\n            total_loss += loss.item()\n    # Normalize the loss.\n    return total_loss / len(valid_dataloader)\n","metadata":{"execution":{"iopub.status.busy":"2022-04-02T19:15:22.015828Z","iopub.execute_input":"2022-04-02T19:15:22.016277Z","iopub.status.idle":"2022-04-02T19:15:22.023138Z","shell.execute_reply.started":"2022-04-02T19:15:22.016239Z","shell.execute_reply":"2022-04-02T19:15:22.022453Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"def get_dataloader(train_dataset, valid_dataset, batch_size=CONFIG.BATCH_SIZE):\n    # Get the Dataloader for Train and Validation datasets.\n    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n    valid_dataloader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n    return train_dataloader, valid_dataloader\n","metadata":{"execution":{"iopub.status.busy":"2022-04-02T19:15:24.738837Z","iopub.execute_input":"2022-04-02T19:15:24.739312Z","iopub.status.idle":"2022-04-02T19:15:24.743479Z","shell.execute_reply.started":"2022-04-02T19:15:24.739274Z","shell.execute_reply":"2022-04-02T19:15:24.742842Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# Store train and valid losses after each epoch.\ntrain_losses = []\nvalid_losses = []\ndef train():\n    # Get the train and valid dataloader\n    train_dataloader, valid_dataloader = get_dataloader(train_dataset, valid_dataset)\n    # Update the best valid loss each\n    best_valid_loss = float(\"inf\")\n    for epoch in range(1, CONFIG.EPOCHS + 1):\n        # Get the train loss for the epoch.\n        train_loss = train_one_epoch(model, optimizer, scheduler, train_dataloader, device)\n        # Get the validation loss for the epoch.\n        valid_loss = valid_one_epoch(model, valid_dataloader, device)\n        # Store the train and valid loss in the list.\n        train_losses.append(train_loss)\n        valid_losses.append(valid_loss)\n        print(f\"Epoch {epoch}: Training Loss: {train_loss:.4f}, Validation Loss: {valid_loss:.4f}\")\n        # Update the best valid loss each time a model gets better performance.\n        if valid_loss < best_valid_loss:\n            best_valid_loss = valid_loss\n            torch.save(model.state_dict(), \"best_model.bin\")    \n","metadata":{"execution":{"iopub.status.busy":"2022-04-02T19:15:28.736968Z","iopub.execute_input":"2022-04-02T19:15:28.737453Z","iopub.status.idle":"2022-04-02T19:15:28.745926Z","shell.execute_reply.started":"2022-04-02T19:15:28.737411Z","shell.execute_reply":"2022-04-02T19:15:28.743330Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"train()","metadata":{"execution":{"iopub.status.busy":"2022-04-02T19:29:58.070401Z","iopub.execute_input":"2022-04-02T19:29:58.071141Z","iopub.status.idle":"2022-04-02T19:55:44.323679Z","shell.execute_reply.started":"2022-04-02T19:29:58.071097Z","shell.execute_reply":"2022-04-02T19:55:44.322827Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"### Train Loss = 0.6576\n### Validation Loss= 0.6454","metadata":{}}]}